deploy.environment = dev
deploy.environment = ${?DEPLOY_ENVIRONMENT}

# Settings for retrying failed stage runs
retry {
  # Delay between retries
  delay = 20 minutes
  delay = ${?AMADOU_RETRY_DELAY}

  # Maximum number of retries
  max = 3
  max = ${?AMADOU_RETRY_MAX}
}

# Example Slack configuration
#slack {
#  url = "https://hooks.slack.com/services/CHANGE-ME"
#  channel = CHANGE-ME
#  user = CHANGE-ME
#  icon = ":ghost:"
#}

# Default settings for Kafka-based monitoring
kafka {
  # Configure the servers to enable Kafka:
  #bootstrap.servers = "host1:1234,host2:1234,host3:1234"
  acks = "all"
  retries = 0
  batch.size = 16384
  linger.ms = 1
  buffer.memory = 33554432
  key.serializer = "org.apache.kafka.common.serialization.StringSerializer"
  value.serializer = "org.apache.kafka.common.serialization.StringSerializer"
  topic.prefix = spark-etl-${deploy.environment}
}

spark.master = "local[*]"
spark.logConf = true

# AWS S3 Credentials
#
# If the AWS_CREDENTIALS environment variable is defined, credentials
# are read that file, however, they can also be overridden using
# environment variables.
spark.hadoop.fs.s3a.access.key = ${?AWS_ACCESS_KEY_ID}
spark.hadoop.fs.s3a.secret.key = ${?AWS_SECRET_ACCESS_KEY}
