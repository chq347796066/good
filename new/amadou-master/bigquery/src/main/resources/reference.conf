# Based on the Google Cloud Storage settings documented here:
# https://cloud.google.com/hadoop/google-cloud-storage-connector#configuringhadoop
spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
spark.hadoop.fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS
spark.hadoop.fs.gs.auth.service.account.json.keyfile = ${?GOOGLE_APPLICATION_CREDENTIALS}
spark.hadoop.fs.gs.project.id = ${?GOOGLE_PROJECT_ID}

# NOTE: Requires distributed file system to make credential files available
spark.hadoop.mapred.bq.auth.service.account.json.keyfile = ${?GOOGLE_APPLICATION_CREDENTIALS}
spark.hadoop.mapred.bq.project.id = ${?GOOGLE_PROJECT_ID}

# Default GCS bucket for uploading to BigQuery
spark.hadoop.mapred.bq.gcs.bucket = ${?GOOGLE_BQ_BUCKET}

# Google OAuth Credentials
spark.google.cloud.auth.client.file = ${?GOOGLE_OAUTH_CREDENTIALS}
